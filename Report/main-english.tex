%Template by Mark Jervelund - 2015 - mjerv15@student.sdu.dk

\documentclass[a4paper,10pt,titlepage]{report}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{ marvosym }
\usepackage{algpseudocode}
\usepackage[document]{ragged2e}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{datenumber}
\usepackage{venndiagram}
\usepackage{chngcntr}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{multirow}

\usepackage{mathtools} % Bonus

\newtheorem{theorem}{Theorem}

\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}

\addbibresource{bibliography.bib}


\DeclarePairedDelimiter\norm\lVert\rVert
\setdatetoday
\addtocounter{datenumber}{0} %date for dilierry standard is today
\setdatebynumber{\thedatenumber}
\date{}
\setcounter{secnumdepth}{0}
\pagestyle{fancy}
\fancyhf{}
\title{Masters Thesis}

\newcommand{\Z}{\mathbb{Z}}
\lhead{Masters Thesis}
\rhead{Mark Jervelund}
\rfoot{Page  \thepage \, of \pageref{LastPage}}
\counterwithin*{equation}{section}




\begin{document}
\begin{titlepage}
\centering
    \vspace*{9\baselineskip}
    \huge
    \bfseries
    Jepsen methods usage for ACID compliance in Hyperscale Cloud Frameworks \\
    \normalfont
    Mark Jervelund \\
    Mark@jervelund.com \\
    Doommius.com \\
    \vspace*{9\baselineskip}
    \normalfont
	\includegraphics[scale=1]{Report/logos/SDU_BLACK.png}
    \vfill\
    \vspace{5mm}
    Institute Of Mathematics and Computer Science, SDU \\

	%Date
    \textbf{\datedate} \\[2\baselineskip]
\end{titlepage}

\renewcommand{\thepage}{\roman{page}}% Roman numerals for page counter
\tableofcontents
\newpage
\setcounter{page}{1}
\renewcommand{\thepage}{\arabic{page}}


\section*{Abstract}

Databases allow modern society to store, manage and distribute data at a previously unprecedentedly scale. having data is a normal practice for most businesses, but it is often done monotonically on a single node, where modern needs require much high performance, storage, latency and  availability then current systems allow. Distributing a data store in a manner that scales performance, storage, availability and desire properties is no easy feat. In this Thesis i present methods of verifying these properties and investigate how different frameworks solve this issue, or often workaround the issue in a way that aligns with the requirements of the systems. \\


Within the framework of the thesis i present methods of verifying the ACID properties, How these properties were solved in the past, How it can and is solved today, what trade offs some systems have made, and a analysis of modern database uses cases and whether they require the ACID properties to solve issues.\\
\vspace{5mm}
First, I Present ACID \& Base, their constraints, what faults occur in database systems, how their manifests themselves, whats the underlying causes often are, and what limits the ACID properties imposes on a given system. Here Base will be introduced to explain what relaxations are introduced to a system where gaining the desire results for certain use cases. \\

Then, I Present Jepsen, a CLojure framework\cite{jepsonio} developed by K. Kingsbury that allow for testing of distributed systems, This is solved via a direct serialization graph (DSG) and queries to the target system that are carefully chosen that allows for traceability, and recoverability.  \\


Then, I present Azure Service Fabric(SF), SF is a distributed container orchestration system made my Microsoft, that allows for hosting of services, or apps. It includes a few different built in functions, but the primary interest here lie within the reliable containers aspect of SF, and what claims Microsoft makes concerning the behaviour of this data.\\

Then, I will present what modern database systems promise, what properties of ACID they follow, which they relax, and which they disregard, and well as what they gain, and which tradeoffs they suffer, Here a main focus will be on Service Fabric.\\

Then I will present the attempt to implement and execute a Jepsen test against service Fabrics reliable containers, and compare these results to the claims Microsoft has made.\\

Finally, I will present how the ACID properties compare to modern uses cases of databases, do we need to follow them strictly or can simply disregard them in some uses cases? What are the exceptions, and what is there to gain ?\\


\chapter{Introduction}

Highly Distributed systems are becoming more and more common as the world grows evermore interconnected, faster paced, increasing data driven with higher expectations of services and product. where the end user expects reliability, availability combined with low- latency and cost, How do we guarantee this is a distributed system that spans the planet. Is it even possible to get both, and is it even needed in most use cases?\\

Most people are familiar with how YouTube, Facebook, Google and Reddit behave. What quirks they sometimes present, Most people remember the YouTube video counter that got stuck around 300, and didn't update for a while, The reason it got stuck was induced, however the number it got stuck on was not, The system was designed to stop at 301, but often went higher. This is an example of the system not being entirely consistent, most nodes where in the correct state of stopping counting when the number got larger then 300, however as some nodes around the globe weren't in an consistent state these still counted the views onto the displayed counter when they were supposed to have been sent to a different table that verified the views before considering them legitimate. This number is still not consistent around the world however for human use cases the issue of a comment or view being a few seconds or ½ a minute delayed isn't something we notice or care about. a lot of user facing services aren't required to be ACID compliant, however there are use cases where this can have an impact is when dealing with data that is produced and consumed in real time, if this is finance, automated or autonomous systems, or other areas where having consistent, atomic and isolated transactions are of the utmost importance. these cases are where it doesn't matter what server we are querying the information from it has to be the most recent sample, and if we at any point are able to query stale data we end up being at the risk of either making an illegal transaction or an autonomous system making a wrong decision, This can a trading system fetching stale data and making the wrong transaction, a bank withdrawal that might be overdrawing the users account. or autonomous vehicle that might making decisions with stale data that can lead to fatal accidents. \\

How do we test and verify that the systems that handle tasks that require compliance with ACID also do so. To understand this we first need to understand the basis for and expected behavior of ACID and the different levels of each of the properties of ACID.


\section{Database transaction models}

Database and data store models can be categories into two main groups, ACID and BASE. Consistent or available. The underlying reason for this limitation is either latency between nodes in a system, scale-ability both in terms of storage or Capacity in terms of queries to the system. Both models of designing the system has it's trade-offs which will be presented later in the paper.
\subsection{ACID}

The ACID Model of handling database transactions is considered monolithic by some, however they still serve a vital a critical function for handling critical systems that require atomicity, consistency across the entire data-set, reliability, and durability 
The ACID acronym is defined as follows in the DBMS book\cite{DBMSbook}.

\begin{itemize}
    \item “A” stands for “atomicity,” the all-or-nothing execution of transactions.
    \item “C,” stands for “consistency.” That is, all databases
have consistency constraints, or expectations about relationships among
data elements (e.g., account balances may not be negative after a transaction finishes). Transactions are expected to preserve the consistency of
the database.
\item “I” stands for “isolation,” the fact that each transaction must appear
to be executed as if no other transaction is executing at the same
time.
\item “D” stands for “durability,” the condition that the effect on the
database of a transaction must never be lost, once the transaction
has completed.
\end{itemize}

ACID therefor offers strong consistency with rigorous handling of transaction isolation that prevent inaccurate data. This allows for designing of a system, where we can prevent operations on stale data, data loss, or "illegal" transactions. These faults will be presented further down in the paper.


\subsection{BASE}
The BASE model allows designing of a system where we value availability, throughput and scale-ability. This can cause issues with stale data, dirty reads, overwriting data, and other undesired behavior. This can benefit some applications where overwriting old data isn't an issue or where the newest version of data might not be required as long as it'll come eventually. The use for these databases are hugely beneficial for social media, logging, and other hyperscale system where consistency isn't required.\\

The Base Acronym was defined by Eric Brewer\cite{brewer2000towards} and is defined as follows. 

\begin{itemize}
    \item Basically Available – Rather than enforcing immediate consistency, BASE-modelled NoSQL databases will ensure availability of data by spreading and replicating it across the nodes of the database cluster.
    \item Soft State – Due to the lack of immediate consistency, data values may change over time. The BASE model breaks off with the concept of a database which enforces its own consistency, delegating that responsibility to developers.
    \item Eventually Consistent – The fact that BASE does not enforce immediate consistency does not mean that it never achieves it. However, until it does, data reads are still possible (even though they might not reflect the reality).
\end{itemize}

The BASE model of databases often have a weak consistency where stale data is considered "OK" so to say, while offering a best effort approach with approximate answers. but gains availability, performance, and it can be considered a relaxed way of handing the database side of things where the system database system is simpler and easier to modify the schema..

\subsection{CAP theorem}

The CAP Theorem was defined by Eric Brewer\cite{CAP} where it states that it is impossible for a distributed database system to provide Consistency, Availability, and Partition Tolerance in a single system, and that only two of these guarantees can be met.\\

It should be noted that the definitions of the terms differ from the definitions in ACID. They are all important when it comes to a distributed systems and their behaviors. firstly Consistency in ACID is define as constraints on the data. By the CAP consistency concept, ACID would follow Sequential consistency as defined by Lamport\cite{lamport1993how}: “the program behaves as if the memory accesses of all processes were interleaved and then executed sequentially.” while the consistency model in CAP is defined as Atomic Consistency (also called linearizability) is sequential plus a real-time constraint: “Unlike sequential consistency, linearizability implicitly assumes the notion of an observable global time across all processes. Operations are modeled by an interval which consists of the period of time between the invocation and response for the operation and each operation is assumed to take effect instantaneously at some point within this interval.” \cite{CSL-TR-95-685}

as CAP states we can only have 2 of the 3 properties in any given data-share system. The three different options will be explained below.

CP Database, A system that deliveries Consistency and Partition tolerance but the trade off here is Availability, If a partition occurs in the system, the non-consistent nodes would have to be shut down or made unavailable to deliver consistent data. CP would cover majority protocols, and most distributed databases. an Example of a CP database would be MongoDB and Service Fabrics Reliable collections, These work by having partitions that contain a master and a set of replicates. The replicates simple follow the masters transaction log and apply it to their own data set. If the Primary becomes unavailable the Replicate with the most recent transaction log simply comes the new Master, doing this switch the partition becomes unavailable while the replicates catches up to the new master. this causes the network to remain consistent but limits availability.  \\

AP If our system foregoes Consistency, and deliverers availability and partition tolerance, In the case of a partition between nodes we keep serving from all nodes but in this case we might serve stale data and occurrence might also occur where the same row contains different values due to multiple different write operations on the different nodes. An example of a AP database would be Cassandra, where the CP have a master/replicate architecture, This would cover DNS, Caching systems and databases such as Cassandra. Cassandra uses a masterless architecture, It does mean that there is multiple points of failure rather then a single one. It is able to be available and partition tolerant but consistency isn't guaranteed as nodes are always available and in case of partitioning the nodes will diverge when partitions and will then heal once they are connected back to the network.   \\

CA Database delivers consistency and availability but doesn't allow for partitions of the network or nodes. this results in a single node, or single cluster system as any distribution of the system introduces network instability and latency that would break the system. In this case it would cover single node/site databases and cluster databases as well as file systems as we in practice wouldn't have a distributed system that doesn't allow for partitioning as it would be unusable. But an example of this would be a single node Database. PostgresSQL could be an example here, however, PostgresSQL dues support replication but then it becomes a CP database, which some asterisks\cite{aphyrpostgres} as it doesn't quite behave as expected in that case either. \\



\subsection{Consistency and consistency models.}


\begin{wrapfigure}{r}{0.5\linewidth}
    \centering
       \includegraphics[scale=0.4]{Report/images/consistency models.PNG}
     \caption{Picture from https://jepsen.io/consistency}
     \label{fig:jepsenioconsistency}
\end{wrapfigure}


For explaining the the concepts within consistency i will use the a paper by Ballis et al on "Highly available transactions"\cite{10.14778/2732232.2732237} that displays a good model for preventing the different consistency models and their relation to other consistency models.\\


\subsubsection{Strict Serializability}

For a system to be Strict Serializatbility, it is required that the entire system operationally appear to occur in the order, with regards to both the order and the real time of the operations.    

Formally Strict Serializatbility is define as as Serviceable system that is compatible with a Time dependent order.
"A history is serializable if it is equivalent to one in which transactions appear to execute sequentially, i.e., without interleaving. A (partial) precedence order can be defined on non-overlapping pairs of transactions in the obvious way. A history is strictly serializable if the transactions’ order in the sequential history is compatible with their precedence order."\cite{10.1145/78969.78972}
Here it should be clarified that the Obvious way that that if we have transaction A and B, That A proceeds B if A Completes Prior to Transaction B Begins´ or in other worlds a Serialzable system with the time constraint from Linearizability.

\subsubsection{Serialzable}

Serializability defines systems where transactions occur in some total order, It is formally defined in the ANSI SQL 1999 spec as follows. "The execution of concurrent SQL-transactions at isolation level SERIALIZABLE is guaranteed to be serializable. A serializable execution is defined to be an execution of the operations of concurrently executing SQL-transactions that produces the same effect as some serial execution of those same SQL-transactions. A serial execution is one in which each SQL-transaction executes to completion before the next SQL-transaction begins."\cite{ansisql1999}

\subsubsection{Repeatable Read}

\subsubsection{Cursor Stability}

\subsubsection{Read committed}

\subsubsection{Read Uncommitted}

\subsubsection{Snapshot isolation}

\subsubsection{Monotonic atomic view}

\subsubsection{Linearizable}
\subsubsection{Sequential}
\subsubsection{Casual}
\subsubsection{Writes Follows Reads}
\subsubsection{Monotonic Reads}
\subsubsection{Monotonic Writes}
\subsubsection{Read your writes}

Two of the consistency models were already mentioned in the previous section namely Atomic Consistency and Sequential consistency, There exist 2 more within common use, these two are Causal+ Consistency and  Eventual Consistency. They differ in way the consistency is handled by all end up in the same state eventually.

\subsection{Isolation levels}

The Isolation level as defined by the ISO/IEC 9075 standard which defines SQL, This means that any database system should present to what level that they follow the isolation levels. in this we have 4 levels that can lead to unintended behavior within a single transaction. 

The 4 Isolation levels are defined as:
read uncommitted
       The read uncommitted does not issue shared lock, which allows session S2 write access to the data S1 is reading.\textcolor{red}{write more in dept} 
 read commited
 \textcolor{red}{write more in dept} 
repeatable read
\textcolor{red}{write more in dept} 
serializable
\textcolor{red}{write more in dept} 



First the 3 types of behavior is explained as these are used to explain the 4 levels of isolation. the are categorized as the 3 below.
\begin{itemize}
\item dirty read,
\item Non-repeatable read and
\item Phantom read
\end{itemize}

From this knowledge a table can be made that gives us an overview of the isolation levels and what behaviour and the trade off they have.
Isolation level	Read phenomena
Dirty read	Non-repeatable read	Phantom read
read uncommitted	yes	yes	yes
read committed	no	yes	yes
repeatable read	no	no	y
serializable	no	no	no
\textcolor{red}{Maybe also include something about snapshot isolation} 

\subsection{Faults and manifistation}

'Dirty read' is when a S1 can read data that S2 has written but not yet committed, It is considered dirty as S2 can rollback the transaction where S1 read data that must be considered non existent. \textcolor{red}{Make example diagram} 
\\
The second case of 'Non-repeatable read' is when S1 reads data that is changed by S2 and committed. so if S1 read the some data again they will have changed. this results in two equal select statements returning different results.  \textcolor{red}{Make example diagram} 

Phantom read
The Third and last type is phantom read which is a special case of non-repeatable read that occurs when S1 reads data where a where condition is used to specify what data we want. After this initial read, a second session S2 inserts data that meets S1's where condition and commits the data. When S1 issues a select statement with the same where condition, it finds new records. It is called phantom read because the new records seem to be of phantom origin.
 \textcolor{red}{Make example diagram} 





\chapter{Jepsen}


\subsection{Introduction}
"
Jepsen is an effort to improve the safety of distributed databases, queues, consensus systems  (...) exploring particular systems’ failure modes. In each analysis we explore whether the system lives up to its documentation’s claims.
"\cite{jepsonio}
\\

Going from the above quite Jepsen is a way that allows us to check whether a given database system lives up to the requirements and premises that are given, and in a world where distributed, hyper scale or even planetary scale systems are becoming the norm the way data storage is done is required to behave in an expected manner, the reason that expected manner is used rather than correct manner is that we may allow certain faults to occur and eventual consistency to allow for higher throughput applications and that some data may only be required locally as to consider a transaction complete even in the case where some undesirable conditions occur, but these are tolerated to a certain extent. //

Two different examples of this is the YouTube 301 views that happened around 2015, where the number of views weren't globally distributed right away. while comments and likes were, this meant two things. //

Comments were available globally in real time, while views would also commit globally doing low load times, or off hours to maximize utilization of resources.

another more modern scenario where the real time data distribution and correctness is of the utmost importance is within transactions in the finance industry,  where we only want to consider a financial transaction committed once all shards of this data is replicated and the 'old' balance is non longer available, otherwise we could reach a condition where worst case a client is able to draw a negative balance on their account or where one of the transactions isn't recorded as only one of the two transactions is recorded.//




\subsection{Modern Database systems, and the trade offs they make.}


Atomicity, and Isolation as we often write the data to a arbitrary node, that accepts after the data has been distributed outside of it's rack, availability zone or region. and where the newest timestamp then overrules all other written data without care of data on other nodes which can breaks Atomicity, and Isolation, as a check and set(CAS), or update statement might uses stale data and due to it's newer timestamp any data written in the meantime gets discarded. Solutions here might require waiting for full distribution of any data prior to the query executing, but this would drastically slow down the entire system. This does however not factor in issues related to system faults
"""

\subsection{Historically}

Historically this was done with using a manually defined hand coded set of patterns to check how a system behaves wrt to the isolation levels above, like if some proven invariants hold, or if an anomaly is present in parallel is present by inserting record x and y in two separate transactions and and in two more transactions checking if we can observe x but not y, or y and not x, as this could show how the systems handles long forks and snapshot isolation, and more importantly if it supports it at all.

these checking are generally quite efficient and run in polynomial time, but they only check for a certain patterns and therefor don't give us the larger picture of where issues are present, and only cover a fairly limited set of configurations and isolation levels, and are defined on a system to system basis and no interchangeability is supporter.

This means that this has been a giant scope project where case by case tests are predefined and where we only test for certain scopes, which more importantly mean that tests have been done but the test coverage haven't been perfect and a lot of anomalies are never detected.

\textcolor{red}{Need to add some more to this section, add examples are past test, diagrams, papers etc}

\subsection{State of the art.}

The State of the art is Jepsen, or rather one of the State of the Art projects.

Jepsen supports quite a few different modules but the most power full features goes by the name Elle, inferring isolation anomalies from Experimental Observations. \\

The way this works is be inferring a dependency graph between client side observations and the database version history. This is done by carefully selecting database objects and operations such that the database reads reveals information about the version history. 

this means that Elle is able to reveal any anomaly and provide a concise explanation as to why a fault occurred with information of what conditions were present to cause this fault to occur. Using this information we are able to say how a system behaves, what level of isolation and which behavior we'll see as well as what promises the system makes and how does promises are reflected in reality.

And these things can be manifested in multiple ways, some cases are as mentioned earlier in the paper accepted as eventually consistency and performance is valued higher or some applications where loosing some data points are acceptable to allow for higher write throughput.

and the same does for stale, dirty, non-repeatable and phantom reads aren't an issue. this could be on hyper scale social networks where if you see something that was deleted, or if the newest posts, comments and likes aren't distributed across the all notes, but they will eventually. but this is a trade off that is done to allow for much higher write performance as the chance a user is making chancing to the same page on two different nodes/clusters are minimal and the required performance drop required to facilitate the guarantee that all data is always up to date would mean that the system wouldn't scale at the rate it's required, this also brings up another interesting topic, approximate programming, where a program doesn't always have to return the correct output, but if we can see a magnitude speedup of a program and accept a faulty result rate of a few \%, then the cost saving and capacity increase could very much be worth it. this could be in the case of Netflix and amazon recommencing products and services, it doesn't have to be perfect and maybe sometimes the result is wrong but if it means we're able to serve a lot more customers before having to degrade services or maybe having it as a step in the service degradation levels to support as many users as possible. \\

but a lot of this lay outside the scope of this thesis but it's also a super interesting field that will probably see grow in the next decade.





\subsection{Past jepsen tests}

https://aphyr.com/posts/294-call-me-maybe-cassandra


https://aphyr.com/posts/291-call-me-maybe-zookeeper







\subsection{methods used in Jepsen tests}

\subsection{related work to Jepsen}


K. Kingsbury. Knossos.
https://github.com/jepsen-io/knossos, 2013-2019.

G. Lowe. Testing and Verifying Concurrent Objects.
Concurrency and Computation: Practice and
Experience, 29(4), 2017

J. M. Wing and C. Gong. Testing and Verifying
Concurrent Objects. Journal of Parallel and
Distributed Computing, 17(1-2), 1993.

P. B. Gibbons and E. Korach. Testing shared
memories. SIAM Journal on Computing, 26(4), 1997

S. Burckhardt, C. Dern, M. Musuvathi, and R. Tan.
Line-up: A Complete and Automatic Linearizability
Checker. PLDI ’10, 2010.



\chapter{Modern Datastores}


\section{Service Fabric}

\subsection{Introduction}

Service Fabric (SF) is a presented as a distributed systems platform something akin to Kubernetes (K8S), where the user is able to build, deploy and scale micro services and containers, one of the key points presented by on Azure Service Fabric is that you're able to run stateful services. It's presented by Microsoft as the backbone of their core services and data stores.


\subsection{Structure of the Framework}


\subsection{DataStore}

So as mentioned above Service Fabric Contains stateful services, these use reliable collections that are in memory storage 

\subsection{Design of the Datastore}

The Data-store is designed as independent partitions where the user is in charge of querying the partitions separately for all data to be returned. eg Each leader only has the data in it's partition and is unable to query other partitions for data. This causes the issue of handling data partitioning and querying on the user.

Eg if we have 5 partitions out endpoint is required for query partition 1-5 prior to being able to return data to the client,

Already at this point we are unable to guarantee consistent data as any read/write occurring doing a read of the portion can cause the stale/dirty reads from the clients perspective.

An exsample.

client A wants to read all data from the datastore and starts reading from node 1-5 sequentially. halfway into the transaction Client B writes a value to node 1 and 4. Client A's transaction will now be dirty as it will contain data from a future commit on node 4 and at the same time stale data from node 1



\subsection{Promises}


\subsection{Configuration}


What we are able to tweak


\subsection{What aspects of the service fabric are we  we'd like to test}

Reliable collections.

\subsection{What does service Fabric promise wrt ACID}

https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-reliable-services-reliable-collections-transactions-locks

\section{Cassadra}

\section{Postgres}

\section{Mongodb}


\section{Future Technologies that can have an impact}
 Atomic clocks ?    



\chapter{Performing a Jepses test}

\subsection{Designing the experiment}

\subsection{Development}

\subsection{Issues}

Linux and windows versions being far from interchangeable and applications and services that function on windows/local machines fail crash on deployed cluster

visual studio being unable to load .dmp files to debug crashed applications from the cloud.

Visual studio remote debugger not working

\subsubsection{Azuer}

Configuration issues,

Cluster cecoming unhealty/corrupt and nodes not being configred correctly after a reimage.

\subsection{Introduction}

The Goal of the experiment to to investigate weather Service Fabric is compliance with ACID or not,


\subsection{Tooling}


%LaTeX hints are provided in \cref{chap:latexhints}.

%\blinddocument

\chapter{Related Work}

\chapter{Conclusion and Outlook}
\section*{Conclusion}

\section*{Outlook}

All links were last followed on March 17, 2018.



\newpage
\appendix
\input{Proposal}

\section{Schedule}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}

\begin{tabular}{clll}
\multicolumn{2}{l}{Mark Jervelund thesis plan} &                                           &                  \\
                                & Week 45      &                                             &                                                                                                                            \\
                                & Week 46      &                                             & \multirow{-2}{*}{\text{Study Jensen test}}                                                                                        \\
                                & Week 47      &                                             &                                                                                                                            \\
\multirow{-4}{*}{November}      & Week 48      &                                             & \multirow{-2}{*}{\text{Finish study on Jepsen test}}                                                                              \\
                                & Week 49      &                                             &                                                                                                                            \\
                                & Week 50      &                                             & \multirow{-2}{*}{\text{Study Service Fabric} }                                                                                    \\
                                & Week 51      &                                             &                                                                            \\
                                & Week 52      &                                             & \multirow{-2}{*}{\text{Christmas break}}                                            \\
\multirow{-5}{*}{December}      & Week 53      &                                             &                                                                                                                            \\
                                & week 01      &  \multirow{-10}{*}{\text{Structured study 8 weeks}} & \multirow{-2}{*}{\text{Finish Study Service Fabric} }                                                                             \\
                                & Week 02      &                                             &                                                                                                                            \\
                                & Week 03      &                                             & \multirow{-2}{*}{\text{designing the experiment}}                                                                                 \\
\multirow{-4}{*}{January}       & Week 04      &                                             &                                                                                                                            \\
                                & Week 05      &                                             & \multirow{-2}{*}{\text{designing the experiment} }                                                                                \\
                                & Week 06      &                                             &                                                                                                                            \\
                                & Week 07      &                                             & \multirow{-2}{*}{\text{Execute the experiment}  }                                                                                 \\
\multirow{-4}{*}{February}      & Week 08      &                                             &                                                                                                                            \\
                                & Week 09      &                                             & \multirow{-2}{*}{\text{Analyse the experiment}}                                                                                   \\
                                & Week 10      &                                             &                                                                                                                            \\
                                & Week 11      &  \multirow{-10}{*}{\text{Experiment 10 weeks}}     & \multirow{-2}{*}{\text{discover what the findings from the experiment is}}                                                         \\
                                & Week 12      &                                             &                                                                                                                            \\
\multirow{-5}{*}{March}         & Week 13      &                                             & \multirow{-2}{*}{\text{Running additional experiments if needed  Full on write mode}} \\
                                & Week 14      &                                             &                                                                                                                            \\
                                & Week 15      &                                             & \multirow{-2}{*}{\text{Full on write mode} }                                                                                      \\
                                & Week 16      &                                             &                                                                                                                            \\
\multirow{-4}{*}{April}         & Week 17      &                                             & \multirow{-2}{*}{\text{Full on write mode} }                                                                                      \\
                                & Week 18      &                                             &                                                                                                                            \\
                                & Week 19      &                                             & \multirow{-2}{*}{\text{Full on write mode    Send out thesis for feedback}}             \\
                                & Week 20      &                                             &                                                                                                                            \\
                                & Week 21      & \multirow{-10}{*}{\text{Thesis writing  10 weeks}} & \multirow{-2}{*}{\text{Correction} }                                                                                              \\
\multirow{-5}{*}{May}           & Week 22      &                                             &                                                                                                                            \\
                                & Week 23      & \multirow{-2}{*}{\text{goal}}                      & \multirow{-2}{*}{\text{Hand in}}                                                                                                  \\
                                & Week 24      & \multicolumn{1}{l}{}                        & \multicolumn{1}{l}{}                                                                                                       \\
                                & Week 25      &  \multicolumn{1}{l}{}                       & \multicolumn{1}{l}{}                                                                                                       \\
                                & Week 26      &  \multicolumn{1}{l}{}                       & \multicolumn{1}{l}{}                                                                                                       \\
\multirow{-5}{*}{June}          & Week 27      & \multicolumn{1}{l}{}                        & \multicolumn{1}{l}{}                                                                                                       \\
\multicolumn{1}{l}{}            & Week 28      & \multicolumn{1}{l}{}                        & \multicolumn{1}{l}{}                                                                                                       \\
\multicolumn{1}{l}{}            & Week 29      & \multicolumn{1}{l}{}                        & \multicolumn{1}{l}{}                                                                                                       \\
\multicolumn{1}{l}{}            & Week 30      & \multicolumn{1}{l}{}                        & \multicolumn{1}{l}{}                                                                                                       \\
\multicolumn{1}{l}{}            & Week 31      & \multicolumn{1}{l}{}                        & \multicolumn{1}{l}{}                                                                                                      
\end{tabular}




%\input{latexhints-english}

\pagestyle{empty}

\printbibliography


\chapter{Needs to be relocated}


\section{Things to read(temp list)}

https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf

https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-reliable-services-reliable-collections




\section{Jepsen}

\end{document}
